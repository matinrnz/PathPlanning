{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from IPython.display import Image\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, environment_rows=11, environment_columns=11, ORDER_REWARD=100,\n",
    "                 TERMINAL_PENALTY=-100, STEP_PENALTY=-1, START=(1,5), reward_locs=[(8,2),(2,7),(5,4),(8,7)]):\n",
    "        \n",
    "        self.environment_rows = environment_rows\n",
    "        self.environment_columns = environment_columns\n",
    "        self.ORDER_REWARD = ORDER_REWARD\n",
    "        self.TERMINAL_PENALTY = TERMINAL_PENALTY\n",
    "        self.STEP_PENALTY = STEP_PENALTY\n",
    "        self.START = START\n",
    "        self.reward_locs = reward_locs.copy() # rewards loc is a list, which is mutable, so we need to make a copy\n",
    "                                                # otherwise, the original list will be changed when we change the copy\n",
    "        self.reward_locs.append(START)\n",
    "        self.NUM_ORDERS = len(self.reward_locs)\n",
    "        self.orders_left = self.NUM_ORDERS        \n",
    "\n",
    "        # Call the build_environment function to initialize rewards\n",
    "        self.REWARDS_MAP = self.build_environment(self.reward_locs.copy()) # map of rewards that will not be updated, used for plotting\n",
    "        self.active_rewrads = self.REWARDS_MAP.copy() # copy of rewards map that will be updated as orders are completed\n",
    "        \n",
    "\n",
    "    def build_environment(self, reward_locs):\n",
    "        # Create a 2D numpy array to hold the rewards for each state.\n",
    "        rewards = np.full((self.environment_rows, self.environment_columns), self.TERMINAL_PENALTY)\n",
    "\n",
    "        # Define aisle locations (i.e., white squares) for rows 1 through 9\n",
    "        aisles = {}\n",
    "        aisles[0] = []\n",
    "        aisles[1] = [i for i in range(1, 10)]\n",
    "        aisles[2] = [1, 5, 9]\n",
    "        aisles[3] = [1, 5, 9]\n",
    "        aisles[4] = [i for i in range(1, 10)]\n",
    "        aisles[5] = [1, 5, 9]\n",
    "        aisles[6] = [i for i in range(1, 10)]\n",
    "        aisles[7] = [1, 5, 9]\n",
    "        aisles[8] = [1, 5, 9]\n",
    "        aisles[9] = [i for i in range(1, 10)]\n",
    "        aisles[10] = []\n",
    "\n",
    "        # Set the rewards for all aisle locations (i.e., white squares)\n",
    "        for row_index in range(0, 11):\n",
    "            for column_index in aisles[row_index]:\n",
    "                rewards[row_index, column_index] = self.STEP_PENALTY\n",
    "\n",
    "        # Set the reward for the packaging area (i.e., the goal) to ORDER_REWARD\n",
    "        for loc in reward_locs[:-1]:\n",
    "            rewards[loc[0], loc[1]] = self.ORDER_REWARD\n",
    "        \n",
    "        return rewards\n",
    "\n",
    "    def reset_active_rewards(self):\n",
    "        self.active_rewrads = self.REWARDS_MAP.copy()\n",
    "        self.orders_left = self.NUM_ORDERS    \n",
    "        \n",
    "    def is_terminal_state(self, current_row_index, current_column_index):\n",
    "        if self.active_rewrads[current_row_index, current_column_index] == self.TERMINAL_PENALTY:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def is_order_state(self, current_row_index, current_column_index, alter_rewards=True):\n",
    "        \"\"\"Returns True if the current state is an order state, False otherwise\n",
    "        \n",
    "        Also removes the order from the active rewards map, and updates the number of orders left\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if self.active_rewrads[current_row_index, current_column_index] == self.ORDER_REWARD:\n",
    "            if alter_rewards:\n",
    "                self.active_rewrads[current_row_index, current_column_index] = self.STEP_PENALTY\n",
    "                self.orders_left -= 1\n",
    "                #print(\"Order completed! Orders left: \", self.orders_left)\n",
    "                if self.orders_left == 1:\n",
    "                    self.active_rewrads[self.START[0], self.START[1]] = self.ORDER_REWARD\n",
    "                return True\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def get_starting_location(self):\n",
    "        return self.START[0], self.START[1]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the shape of the environment (i.e., its states)\n",
    "environment_rows = 11\n",
    "environment_columns = 11\n",
    "ORDER_REWARD = 100\n",
    "TERMINAL_PENALTY = -100\n",
    "STEP_PENALTY = -1\n",
    "START = (1,5)\n",
    "reward_locs = [(8,2),(2,7),(5,4),(8,7)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15f0c5a9100>"
      ]
     },
     "execution_count": 1244,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the Environment class\n",
    "env = Environment(environment_rows, environment_columns, ORDER_REWARD, TERMINAL_PENALTY, STEP_PENALTY, START, reward_locs)\n",
    "\n",
    "# You can access the rewards array using env.rewards\n",
    "plt.imshow(env.REWARDS_MAP, cmap='hot', interpolation='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.NUM_ORDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 2), (2, 7), (5, 4), (8, 7), (1, 5)]"
      ]
     },
     "execution_count": 1246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_locs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.is_terminal_state(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Type\n",
    "from tqdm import tqdm\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self,environment_rows=11, environment_columns=11,ORDER_REWARD=100, TERMINAL_PENALTY=-100,\n",
    "                 STEP_PENALTY=-1, START=(1,5), reward_locs=[(8,2),(2,7),(5,4),(8,7)],actions=('up', 'right', 'down', 'left')):\n",
    "        \n",
    "        self.env = Environment(environment_rows, environment_columns,ORDER_REWARD, TERMINAL_PENALTY, STEP_PENALTY, START, reward_locs.copy())\n",
    "        # Initialize the Q-values to random numbers between -0.1 and 0.1\n",
    "        self.q_values = np.random.random((self.env.NUM_ORDERS, environment_rows, environment_columns, 4)) * 2 - 1\n",
    "        self.q_values = self.q_values / 10\n",
    "        self.environment_rows = environment_rows\n",
    "        self.environment_columns = environment_columns\n",
    "        self.actions = actions\n",
    "\n",
    "\n",
    "    def get_next_action(self, current_row_index, current_column_index, epsilon, order_number):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.argmax(self.q_values[order_number, current_row_index, current_column_index])\n",
    "        else:\n",
    "            return np.random.randint(4)\n",
    "\n",
    "    def get_next_location(self, current_row_index, current_column_index, action_index):\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.environment_columns - 1:\n",
    "            new_column_index += 1\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.environment_rows - 1:\n",
    "            new_row_index += 1\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "        else:\n",
    "            raise Exception(f\"Invalid action {action_index}\")\n",
    "        return new_row_index, new_column_index\n",
    "    \n",
    "    def get_shortest_path(self, verbose=True):\n",
    "        self.env.reset_active_rewards()\n",
    "        shortest_path = []\n",
    "        reward_sum = 0\n",
    "        if self.env.is_terminal_state(self.env.START[0], self.env.START[1]) or self.env.is_order_state(self.env.START[0], self.env.START[1], alter_rewards=False):\n",
    "            print(f\"START: {self.env.START} | Terminal State: {self.env.is_terminal_state(self.env.START[0], self.env.START[1])} | Order State: {self.env.is_order_state(self.env.START[0], self.env.START[1], alter_rewards=False)}\")\n",
    "            print(f\"Rwards: {self.env.active_rewrads[self.env.START[0], self.env.START[1]]}\")\n",
    "            raise Exception(\"The starting location cannot be a terminal state or an order state!\")\n",
    "        current_row_index, current_column_index = self.env.get_starting_location()\n",
    "        shortest_path.append((current_row_index, current_column_index))\n",
    "        reward_sum += self.env.active_rewrads[current_row_index, current_column_index]\n",
    "        while not self.env.is_terminal_state(current_row_index, current_column_index) and self.env.orders_left > 0:\n",
    "            order_number = self.env.NUM_ORDERS - self.env.orders_left # order number is the number of orders completed so far which is the index of the current order and defines the q_values layer to use\n",
    "            action_index = self.get_next_action(current_row_index, current_column_index,1.0, order_number)\n",
    "            current_row_index, current_column_index = self.get_next_location(current_row_index, current_column_index, action_index)\n",
    "            shortest_path.append((current_row_index, current_column_index))\n",
    "            reward_sum += self.env.active_rewrads[current_row_index, current_column_index]\n",
    "            self.env.is_order_state(current_row_index, current_column_index) # this will remove the order from the active rewards map\n",
    "            \n",
    "            if len(shortest_path) > 500:\n",
    "                if verbose:\n",
    "                    print(\"Infinite loop! Something is wrong\")\n",
    "                break\n",
    "        return shortest_path, reward_sum\n",
    "    \n",
    "    def fitness_score(self, verbose=False):\n",
    "        shortest_path, reward_sum = self.get_shortest_path(verbose=verbose)\n",
    "        return reward_sum\n",
    "    \n",
    "    def train(self, num_episodes, epsilon, discount_factor, learning_rate, change_w_gamma=False, verbose = True):\n",
    "        \"\"\" Train the Q-learning agent\n",
    "        Inputs:\n",
    "            num_episodes: Number of episodes to use for training\n",
    "            epsilon: The probability of choosing a random action (between 0 and 1)\n",
    "            discount_factor: Discount factor for future rewards\n",
    "            learning_rate: The step size to use for updating the Q-values\n",
    "            change_w_gamma: If True, the learning rate and epsilon will change with gamma throughout training (default False)\n",
    "        returns:\n",
    "            episode_rewards_list: List containing the rewards obtained in each episode during training\n",
    "        \"\"\"\n",
    "        self.env.reset_active_rewards()\n",
    "        gamma = 1-(1 / (num_episodes/2))\n",
    "        progress_bar = tqdm(range(num_episodes), desc=\"Training Progress\", unit=\"Episode\", disable=not verbose)\n",
    "        zero_order_hit_count = 0\n",
    "        one_order_hit_count = 0\n",
    "        episode_rewards_list = []\n",
    "        for episode in progress_bar:\n",
    "            self.env.reset_active_rewards()\n",
    "            episode_rewards_sum = 0\n",
    "            row_index, column_index = self.env.get_starting_location()\n",
    "            \n",
    "            while not self.env.is_terminal_state(row_index, column_index) and self.env.orders_left > 0:\n",
    "                order_number = self.env.NUM_ORDERS - self.env.orders_left\n",
    "                action_index = self.get_next_action(row_index, column_index,epsilon, order_number=order_number)\n",
    "                \n",
    "                old_row_index, old_column_index = row_index, column_index # store the old row and column indexes\n",
    "                row_index, column_index = self.get_next_location(row_index, column_index, action_index)\n",
    "                \n",
    "                reward = self.env.active_rewrads[row_index, column_index]\n",
    "                episode_rewards_sum += reward\n",
    "                \n",
    "                old_q_value = self.q_values[order_number, old_row_index, old_column_index, action_index]\n",
    "                temporal_difference = reward + (discount_factor * np.max(self.q_values[order_number, row_index, column_index])) - old_q_value\n",
    "                new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "                self.q_values[order_number, old_row_index, old_column_index, action_index] = new_q_value\n",
    "                \n",
    "                if self.env.is_order_state(row_index, column_index):\n",
    "                    if self.env.orders_left == 0:\n",
    "                        zero_order_hit_count += 1\n",
    "                    elif self.env.orders_left == 1:\n",
    "                        one_order_hit_count += 1\n",
    "                \n",
    "            episode_rewards_list.append(episode_rewards_sum)\n",
    "            \n",
    "            if change_w_gamma:\n",
    "                learning_rate = max(learning_rate * gamma, 0.01)\n",
    "                epsilon = min(epsilon * (2-gamma), 0.9)\n",
    "\n",
    "            progress_bar.set_postfix({\"Ord left\": self.env.orders_left, \"lr\": learning_rate, \"Eps\": epsilon,\n",
    "                                      \"0 Order\":zero_order_hit_count, \"1 Order\":one_order_hit_count})\n",
    "        \n",
    "        progress_bar.close()\n",
    "        return episode_rewards_list\n",
    "            \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 11, 11, 4)"
      ]
     },
     "execution_count": 1249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = QLearningAgent(environment_rows, environment_columns, ORDER_REWARD, TERMINAL_PENALTY, STEP_PENALTY, START, reward_locs)\n",
    "agent.q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 500/500 [00:00<00:00, 1346.95Episode/s, Ord left=5, lr=0.9, Eps=0.9, 0 Order=0, 1 Order=50]\n"
     ]
    }
   ],
   "source": [
    "reward_per_episode_list = agent.train(num_episodes=500, epsilon=0.9, discount_factor=0.9, learning_rate=0.9, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-97"
      ]
     },
     "execution_count": 1251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.fitness_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15f0c634100>"
      ]
     },
     "execution_count": 1252,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(agent.env.active_rewrads, cmap='hot', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 1253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.env.ORDER_REWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infinite loop! Something is wrong\n"
     ]
    }
   ],
   "source": [
    "shortest_path = agent.get_shortest_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:  [(1, 5), (1, 6), (1, 7), (2, 7), (1, 7), (1, 6), (1, 5), (2, 5), (3, 5), (4, 5), (4, 4), (5, 4), (6, 4), (6, 3), (6, 2), (6, 1), (7, 1), (8, 1), (8, 2), (9, 2), (9, 3), (9, 4), (9, 5), (9, 6), (9, 7), (8, 7), (9, 7), (9, 6), (9, 5), (9, 4), (9, 3), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2), (9, 2), (8, 2)] \n",
      "RewardSum:  -97\n"
     ]
    }
   ],
   "source": [
    "print(\"Path: \",shortest_path[0], \"\\nRewardSum: \", shortest_path[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneticTraining:\n",
    "    def __init__(self, population=10, num_episodes=1000, epsilon=0.9, discount_factor=0.9, learning_rate=0.1):\n",
    "        self.population = population\n",
    "        self.num_episodes = num_episodes\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.agents = [QLearningAgent() for _ in range(population)]\n",
    "        self.agent_q_table_shape = self.agents[0].q_values.shape\n",
    "        self.best_agent = None\n",
    "        self.best_agent_reward = 0\n",
    "        self.best_agent_index = None\n",
    "        self.best_agent_shortest_path = None\n",
    "    \n",
    "    def agents_fitness_score(self, agents):\n",
    "        fitness_list = [agent.fitness_score() for agent in agents]\n",
    "        return fitness_list.copy()\n",
    "    \n",
    "    def selection(self, agents):\n",
    "        \"\"\"commonly known as roulette wheel selection or fitness-proportionate selection\n",
    "        in a genetic algorithm. It is a way to select individuals from a population with\n",
    "        a probability that is proportional to their fitness scores.\"\"\"\n",
    "        fitness_list = self.agents_fitness_score(agents)\n",
    "        fitness_array = np.array(fitness_list, dtype=np.float64)\n",
    "        if np.min(fitness_array) < 0: # to avoid negative fitness values, which consequently will cause negative probabilities.\n",
    "            fitness_array -= (np.min(fitness_array) * 2)\n",
    "            \n",
    "        if np.sum(fitness_array) == 0:\n",
    "            fitness_array += 0.001\n",
    "        normalized_probabilities = fitness_array / (np.sum(fitness_array)) # add a small value to avoid zero probabilities\n",
    "        parents = np.random.choice(agents, size=2, replace=False, p=normalized_probabilities)\n",
    "        return parents\n",
    "    \n",
    "    def crossover(self, parents)-> QLearningAgent:\n",
    "        child = QLearningAgent()\n",
    "        crossover_point = np.random.randint(2, size = self.agent_q_table_shape[:-1])\n",
    "        crossover_point = np.stack([crossover_point] * self.agent_q_table_shape[-1], axis=-1)\n",
    "        child_q_values = np.where(crossover_point == 1, parents[0].q_values, parents[1].q_values)\n",
    "        child.q_values = child_q_values\n",
    "        return child\n",
    "    \n",
    "    def mutation(self, child: Type[QLearningAgent], mutation_prob=0.1)-> QLearningAgent:\n",
    "        mutation_point = np.random.choice([0,1], size = self.agent_q_table_shape[:-1], p=[1-mutation_prob, mutation_prob])\n",
    "        mutation_point = np.stack([mutation_point] * self.agent_q_table_shape[-1], axis=-1)\n",
    "        \n",
    "        max_gene_value = np.max(child.q_values)\n",
    "        min_gene_value = np.min(child.q_values)\n",
    "        \n",
    "        print(f\"max_gene_value: {max_gene_value} | min_gene_value: {min_gene_value}\")\n",
    "        \n",
    "        mutation_value = np.random.uniform(low=min_gene_value, high=max_gene_value, size=self.agent_q_table_shape)\n",
    "        child.q_values = np.where(mutation_point == 1, mutation_value, child)\n",
    "        return child\n",
    "    \n",
    "    def genetic_algorithm(self, agents_list, parents_to_keep_ratio=0.5, mutation_prob=0.1):\n",
    "        agents_list = sorted(agents_list.copy(), key=lambda agent: agent.fitness_score(), reverse=True)\n",
    "        num_parents_to_keep = int(len(agents_list) * parents_to_keep_ratio)\n",
    "        num_childs_to_create = len(agents_list) - num_parents_to_keep\n",
    "        childs_list = []\n",
    "        for _ in range(num_childs_to_create):\n",
    "            parents = self.selection(agents_list)\n",
    "            child = self.crossover(parents)\n",
    "            # child = self.mutation(child, mutation_prob)\n",
    "            childs_list.append(child)\n",
    "            \n",
    "        agents_list = agents_list[:num_parents_to_keep] + childs_list\n",
    "        \n",
    "        return agents_list\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self, episodes = 500, q_learning_episodes = 10):\n",
    "        if episodes % q_learning_episodes != 0:\n",
    "            raise Exception(\"episodes must be divisible by q_learning_step\")\n",
    "        # call the .tarin() method of each agent for q_learning_step times\n",
    "        steps = episodes // q_learning_episodes\n",
    "        progress_bar = tqdm(range(steps), desc=\"Training Progress\", unit=\"Step\")\n",
    "        for step in progress_bar:\n",
    "            for agent in self.agents:\n",
    "                agent.train(num_episodes=q_learning_episodes, epsilon=self.epsilon,\n",
    "                            discount_factor=self.discount_factor, learning_rate=self.learning_rate, verbose=False)\n",
    "                \n",
    "            self.agents = self.genetic_algorithm(self.agents)\n",
    "            \n",
    "            progress_bar.set_postfix({\"Best Reward\": self.agents[0].fitness_score(verbose=False)})\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 2, 3]\n",
      "[5, 5, 6]\n",
      "[5, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5, 2, 3]"
      ]
     },
     "execution_count": 1257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class test:\n",
    "    def __init__(self, lst=[1,2,3]):\n",
    "        self.lst = lst.copy()\n",
    "    def change_list(self):\n",
    "        self.lst[0] = 5\n",
    "        \n",
    "list_of_lists = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "t1 = test(list_of_lists[0])\n",
    "t2 = test(list_of_lists[1])\n",
    "t3 = test(list_of_lists[2])\n",
    "for t in [t1,t2,t3]:\n",
    "    t.change_list()\n",
    "    print(t.lst)\n",
    "t1.lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 50/50 [00:04<00:00, 10.95Step/s, Best Reward=-97] \n"
     ]
    }
   ],
   "source": [
    "gt = GeneticTraining()\n",
    "gt.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-97"
      ]
     },
     "execution_count": 1268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.agents[8].fitness_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# child_test =(gt.crossover(gt.selection(gt.agents)))\n",
    "# child_test.train(num_episodes=100, epsilon=0.9, discount_factor=0.9, learning_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.QLearningAgent at 0x15f0c5c4bb0>,\n",
       " <__main__.QLearningAgent at 0x15f0c54f670>,\n",
       " <__main__.QLearningAgent at 0x15f0c64a070>,\n",
       " <__main__.QLearningAgent at 0x15f0c665b80>,\n",
       " <__main__.QLearningAgent at 0x15f0c665c40>,\n",
       " <__main__.QLearningAgent at 0x15f0c6344f0>,\n",
       " <__main__.QLearningAgent at 0x15f0c138f70>,\n",
       " <__main__.QLearningAgent at 0x15f0c665a30>,\n",
       " <__main__.QLearningAgent at 0x15f0c5a9b50>,\n",
       " <__main__.QLearningAgent at 0x15f0c665880>]"
      ]
     },
     "execution_count": 1261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.genetic_algorithm(gt.agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
