{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from IPython.display import Image\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, environment_rows, environment_columns, ORDER_REWARD, TERMINAL_PENALTY, STEP_PENALTY, START, reward_locs):\n",
    "        self.environment_rows = environment_rows\n",
    "        self.environment_columns = environment_columns\n",
    "        self.ORDER_REWARD = ORDER_REWARD\n",
    "        self.TERMINAL_PENALTY = TERMINAL_PENALTY\n",
    "        self.STEP_PENALTY = STEP_PENALTY\n",
    "        self.START = START\n",
    "        self.reward_locs = reward_locs\n",
    "        self.reward_locs.append(START)\n",
    "        self.NUM_ORDERS = len(reward_locs)\n",
    "        self.orders_left = self.NUM_ORDERS        \n",
    "\n",
    "        # Call the build_environment function to initialize rewards\n",
    "        self.REWARDS_MAP = self.build_environment(reward_locs) # map of rewards that will not be updated, used for plotting\n",
    "        self.active_rewrads = self.REWARDS_MAP.copy() # copy of rewards map that will be updated as orders are completed\n",
    "        \n",
    "\n",
    "    def build_environment(self, reward_locs):\n",
    "        # Create a 2D numpy array to hold the rewards for each state.\n",
    "        rewards = np.full((self.environment_rows, self.environment_columns), self.TERMINAL_PENALTY)\n",
    "\n",
    "        # Define aisle locations (i.e., white squares) for rows 1 through 9\n",
    "        aisles = {}\n",
    "        aisles[0] = []\n",
    "        aisles[1] = [i for i in range(1, 10)]\n",
    "        aisles[2] = [1, 5, 9]\n",
    "        aisles[3] = [1, 5, 9]\n",
    "        aisles[4] = [i for i in range(1, 10)]\n",
    "        aisles[5] = [1, 5, 9]\n",
    "        aisles[6] = [i for i in range(1, 10)]\n",
    "        aisles[7] = [1, 5, 9]\n",
    "        aisles[8] = [1, 5, 9]\n",
    "        aisles[9] = [i for i in range(1, 10)]\n",
    "        aisles[10] = []\n",
    "\n",
    "        # Set the rewards for all aisle locations (i.e., white squares)\n",
    "        for row_index in range(0, 11):\n",
    "            for column_index in aisles[row_index]:\n",
    "                rewards[row_index, column_index] = self.STEP_PENALTY\n",
    "\n",
    "        # Set the reward for the packaging area (i.e., the goal) to ORDER_REWARD\n",
    "        for loc in reward_locs[:-1]:\n",
    "            rewards[loc[0], loc[1]] = self.ORDER_REWARD\n",
    "        \n",
    "        return rewards\n",
    "\n",
    "    def reset_active_rewards(self):\n",
    "        self.active_rewrads = self.REWARDS_MAP.copy()\n",
    "        self.orders_left = self.NUM_ORDERS    \n",
    "        \n",
    "    def is_terminal_state(self, current_row_index, current_column_index):\n",
    "        if self.active_rewrads[current_row_index, current_column_index] == self.TERMINAL_PENALTY:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def is_order_state(self, current_row_index, current_column_index):\n",
    "        \"\"\"Returns True if the current state is an order state, False otherwise\n",
    "        \n",
    "        Also removes the order from the active rewards map, and updates the number of orders left\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if self.active_rewrads[current_row_index, current_column_index] == self.ORDER_REWARD:\n",
    "            self.active_rewrads[current_row_index, current_column_index] = self.STEP_PENALTY\n",
    "            self.orders_left -= 1\n",
    "            #print(\"Order completed! Orders left: \", self.orders_left)\n",
    "            if self.orders_left == 1:\n",
    "                self.active_rewrads[self.START[0], self.START[1]] = self.ORDER_REWARD\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def get_starting_location(self):\n",
    "        return self.START[0], self.START[1]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the shape of the environment (i.e., its states)\n",
    "environment_rows = 11\n",
    "environment_columns = 11\n",
    "ORDER_REWARD = 100\n",
    "TERMINAL_PENALTY = -100\n",
    "STEP_PENALTY = -1\n",
    "START = (1,5)\n",
    "reward_locs = [(8,2),(2,7),(5,4),(8,7)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21b80b12ca0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAWsUlEQVR4nO3df2xV9f3H8deljtvS3F4Hpr9CiyVpUqT+wJYZoQpGbaKEjC0Rf4Aj8o/EAq1NXOlwU1noHWwjJHaWlD+QhRRrsoEsmZuNm60EiaVQJWyBMAm9kTWNC7m3VLyE9vP9w3D5XlqryLm8722fj+T80dPTnndubu4zn97Tc33OOScAAAxMsR4AADB5ESEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGDmFusBrjUyMqJz584pEAjI5/NZjwMAuE7OOQ0ODqqwsFBTpoy/1km5CJ07d05FRUXWYwAAblA4HNbMmTPHPSblIhQIBCRJmZJYBwFA+nGSvtLV1/PxpFyErvwJziciBADp7Lu8pcKFCQAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNJi9Abb7yhkpISZWZmqqKiQh9++GGyTgUASFNJiVB7e7vq6uq0ceNGHTt2TA888IAee+wx9fX1JeN0AIA05XPOOa9/6X333ad7771XLS0t8X1z5szRsmXLFAqFxv3ZaDSqYDCoLHHvOABIR07SRUmRSEQ5OTnjHuv5SujSpUvq6elRdXV1wv7q6modOnRo1PGxWEzRaDRhAwBMDp5H6IsvvtDw8LDy8vIS9ufl5am/v3/U8aFQSMFgML7xWUIAMHkk7cKEa2/h7Zwb87bejY2NikQi8S0cDidrJABAivH884Ruu+02ZWRkjFr1DAwMjFodSZLf75ff7/d6DABAGvB8JTR16lRVVFSoo6MjYX9HR4cWLFjg9ekAAGksKZ+sWl9fr2effVaVlZW6//771draqr6+Pq1ZsyYZpwMApKmkROjJJ5/U//73P23atEn//e9/VV5err/+9a+aNWtWMk4HAEhTSfk/oRvB/wkBQHoz/T8hAAC+KyIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwk5R7x00UQ8utJwCA65P9tvUE14eVEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmbrEeAOPLftt6AlxraLn1BF9LhefGkHPWI0iSsn0+6xEkpc5zI52wEgIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZzyMUCoU0f/58BQIB5ebmatmyZTp58qTXpwEATACeR6izs1M1NTU6fPiwOjo6dPnyZVVXV2toaMjrUwEA0pznnyf0t7/9LeHrXbt2KTc3Vz09PXrwwQe9Ph0AII0l/UPtIpGIJGn69Oljfj8WiykWi8W/jkajyR4JAJAiknphgnNO9fX1qqqqUnl5+ZjHhEIhBYPB+FZUVJTMkQAAKSSpEVq7dq0+/fRT7d279xuPaWxsVCQSiW/hcDiZIwEAUkjS/hy3bt06HThwQF1dXZo5c+Y3Huf3++X3+5M1BgAghXkeIeec1q1bp3379umDDz5QSUmJ16cAAEwQnkeopqZGbW1teueddxQIBNTf3y9JCgaDysrK8vp0AIA05vl7Qi0tLYpEIlq8eLEKCgriW3t7u9enAgCkuaT8OQ4AgO+Ce8cBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMJP1D7QBMXNk+n/UISHOshAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwc4v1ABjf0HLrCZCqeG5gImAlBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNJj1AoFJLP51NdXV2yTwUASDNJjVB3d7daW1t11113JfM0AIA0lbQIXbhwQStWrNDOnTv1wx/+MFmnAQCksaRFqKamRkuWLNEjjzwy7nGxWEzRaDRhAwBMDkn5ZNW33npLR48eVXd397ceGwqF9NprryVjDABAivN8JRQOh1VbW6s9e/YoMzPzW49vbGxUJBKJb+Fw2OuRAAApyuecc17+wv379+snP/mJMjIy4vuGh4fl8/k0ZcoUxWKxhO9dKxqNKhgMKkuSz8vBvoeh5cYDAMB1yn7begLJSbooKRKJKCcnZ9xjPf9z3MMPP6zjx48n7HvuuedUVlamhoaGcQMEAJhcPI9QIBBQeXl5wr7s7GzNmDFj1H4AwOTGHRMAAGaScnXctT744IObcRoAQJphJQQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBzU+6YgO8vFe6ImyqGvL3h+/f3pPX93b/GcyP1cOf968dKCABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYOYW6wEwvqHl1hOkkCd91hOkFJ4bmAhYCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZpISoc8//1wrV67UjBkzNG3aNN1zzz3q6elJxqkAAGnM87tonz9/XgsXLtRDDz2kd999V7m5ufrPf/6jW2+91etTAQDSnOcR2rJli4qKirRr1674vttvv93r0wAAJgDP/xx34MABVVZW6oknnlBubq7mzZunnTt3fuPxsVhM0Wg0YQMATA6eR+izzz5TS0uLSktL9fe//11r1qzR+vXr9cc//nHM40OhkILBYHwrKiryeiQAQIryOeecl79w6tSpqqys1KFDh+L71q9fr+7ubn300Uejjo/FYorFYvGvo9GoioqKlCXJ+nM0+eRKAOkm+23rCSQn6aKkSCSinJyccY/1fCVUUFCgO+64I2HfnDlz1NfXN+bxfr9fOTk5CRsAYHLwPEILFy7UyZMnE/adOnVKs2bN8vpUAIA053mEXnzxRR0+fFhNTU06ffq02tra1NraqpqaGq9PBQBIc55HaP78+dq3b5/27t2r8vJy/frXv9b27du1YsUKr08FAEhznl+YcKOi0aiCwSAXJgDA9zDpL0wAAOC7IkIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmPH8k1XhrVT472ckSpU7afDcSD2p8txIJ6yEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZm6xHgDjG3LOegRJUrbPZz0CUhDPT9woVkIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBnPI3T58mW9/PLLKikpUVZWlmbPnq1NmzZpZGTE61MBANKc53fR3rJli3bs2KHdu3dr7ty5OnLkiJ577jkFg0HV1tZ6fToAQBrzPEIfffSRfvzjH2vJkiWSpNtvv1179+7VkSNHvD4VACDNef7nuKqqKr3//vs6deqUJOmTTz7RwYMH9fjjj495fCwWUzQaTdgAAJOD5yuhhoYGRSIRlZWVKSMjQ8PDw9q8ebOefvrpMY8PhUJ67bXXvB4DAJAGPF8Jtbe3a8+ePWpra9PRo0e1e/du/e53v9Pu3bvHPL6xsVGRSCS+hcNhr0cCAKQoz1dCL730kjZs2KCnnnpKknTnnXfq7NmzCoVCWrVq1ajj/X6//H6/12MAANKA5yuhL7/8UlOmJP7ajIwMLtEGAIzi+Upo6dKl2rx5s4qLizV37lwdO3ZM27Zt0+rVq70+FQAgzXkeoddff12//OUv9cILL2hgYECFhYV6/vnn9atf/crrUwEA0pznEQoEAtq+fbu2b9/u9a8GAEww3DsOAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMz4nHPOeoj/LxqNKhgMKkuSz3iWoeXGAwDAdcp+23oCyUm6KCkSiSgnJ2fcY1kJAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGDmFusBUln229YTAMDExkoIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBz3RHq6urS0qVLVVhYKJ/Pp/379yd83zmnV199VYWFhcrKytLixYt14sQJr+YFAEwg1x2hoaEh3X333Wpubh7z+1u3btW2bdvU3Nys7u5u5efn69FHH9Xg4OANDwsAmFh8zjn3vX/Y59O+ffu0bNkySV+vggoLC1VXV6eGhgZJUiwWU15enrZs2aLnn3/+W39nNBpVMBhUliTf9x0MAGDGSbooKRKJKCcnZ9xjPX1P6MyZM+rv71d1dXV8n9/v16JFi3To0KExfyYWiykajSZsAIDJwdMI9ff3S5Ly8vIS9ufl5cW/d61QKKRgMBjfioqKvBwJAJDCknJ1nM+X+Ic059yofVc0NjYqEonEt3A4nIyRAAApyNOP987Pz5f09YqooKAgvn9gYGDU6ugKv98vv9/v5RgAgDTh6UqopKRE+fn56ujoiO+7dOmSOjs7tWDBAi9PBQCYAK57JXThwgWdPn06/vWZM2fU29ur6dOnq7i4WHV1dWpqalJpaalKS0vV1NSkadOm6ZlnnvF0cABA+rvuCB05ckQPPfRQ/Ov6+npJ0qpVq/Tmm2/q5z//uS5evKgXXnhB58+f13333af33ntPgUDAu6kBABPCDf2fUDLwf0IAkN7M/k8IAIDrQYQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMOPpXbS9cOUGDil1GwcAwHd25fX7u9yQJ+UiNDg4KEn6yngOAMCNGRwcVDAYHPeYlLt33MjIiM6dO6dAIPCNH4T3baLRqIqKihQOh7/1vkUTHY9FIh6Pq3gsruKxuMqLx8I5p8HBQRUWFmrKlPHf9Um5ldCUKVM0c+ZMT35XTk7OpH9CXcFjkYjH4yoei6t4LK660cfi21ZAV3BhAgDADBECAJiZkBHy+/165ZVX5Pf7rUcxx2ORiMfjKh6Lq3gsrrrZj0XKXZgAAJg8JuRKCACQHogQAMAMEQIAmCFCAAAzEzJCb7zxhkpKSpSZmamKigp9+OGH1iPddKFQSPPnz1cgEFBubq6WLVumkydPWo+VEkKhkHw+n+rq6qxHMfH5559r5cqVmjFjhqZNm6Z77rlHPT091mOZuHz5sl5++WWVlJQoKytLs2fP1qZNmzQyMmI9WtJ1dXVp6dKlKiwslM/n0/79+xO+75zTq6++qsLCQmVlZWnx4sU6ceKE53NMuAi1t7errq5OGzdu1LFjx/TAAw/oscceU19fn/VoN1VnZ6dqamp0+PBhdXR06PLly6qurtbQ0JD1aKa6u7vV2tqqu+66y3oUE+fPn9fChQv1gx/8QO+++67+9a9/6fe//71uvfVW69FMbNmyRTt27FBzc7P+/e9/a+vWrfrtb3+r119/3Xq0pBsaGtLdd9+t5ubmMb+/detWbdu2Tc3Nzeru7lZ+fr4effTR+P09PeMmmB/96EduzZo1CfvKysrchg0bjCZKDQMDA06S6+zstB7FzODgoCstLXUdHR1u0aJFrra21nqkm66hocFVVVVZj5EylixZ4lavXp2w76c//albuXKl0UQ2JLl9+/bFvx4ZGXH5+fnuN7/5TXzfV1995YLBoNuxY4en555QK6FLly6pp6dH1dXVCfurq6t16NAho6lSQyQSkSRNnz7deBI7NTU1WrJkiR555BHrUcwcOHBAlZWVeuKJJ5Sbm6t58+Zp586d1mOZqaqq0vvvv69Tp05Jkj755BMdPHhQjz/+uPFkts6cOaP+/v6E11K/369FixZ5/lqacjcwvRFffPGFhoeHlZeXl7A/Ly9P/f39RlPZc86pvr5eVVVVKi8vtx7HxFtvvaWjR4+qu7vbehRTn332mVpaWlRfX69f/OIX+vjjj7V+/Xr5/X797Gc/sx7vpmtoaFAkElFZWZkyMjI0PDyszZs36+mnn7YezdSV18uxXkvPnj3r6bkmVISuuPYjIJxz3/tjISaCtWvX6tNPP9XBgwetRzERDodVW1ur9957T5mZmdbjmBoZGVFlZaWampokSfPmzdOJEyfU0tIyKSPU3t6uPXv2qK2tTXPnzlVvb6/q6upUWFioVatWWY9n7ma8lk6oCN12223KyMgYteoZGBgYVfTJYt26dTpw4IC6uro8+4iMdNPT06OBgQFVVFTE9w0PD6urq0vNzc2KxWLKyMgwnPDmKSgo0B133JGwb86cOfrTn/5kNJGtl156SRs2bNBTTz0lSbrzzjt19uxZhUKhSR2h/Px8SV+viAoKCuL7k/FaOqHeE5o6daoqKirU0dGRsL+jo0MLFiwwmsqGc05r167Vn//8Z/3jH/9QSUmJ9UhmHn74YR0/fly9vb3xrbKyUitWrFBvb++kCZAkLVy4cNSl+qdOndKsWbOMJrL15ZdfjvrQtYyMjElxifZ4SkpKlJ+fn/BaeunSJXV2dnr+WjqhVkKSVF9fr2effVaVlZW6//771draqr6+Pq1Zs8Z6tJuqpqZGbW1teueddxQIBOKrw2AwqKysLOPpbq5AIDDqvbDs7GzNmDFj0r1H9uKLL2rBggVqamrS8uXL9fHHH6u1tVWtra3Wo5lYunSpNm/erOLiYs2dO1fHjh3Ttm3btHr1auvRku7ChQs6ffp0/OszZ86ot7dX06dPV3Fxserq6tTU1KTS0lKVlpaqqalJ06ZN0zPPPOPtIJ5ea5ci/vCHP7hZs2a5qVOnunvvvXdSXpYsacxt165d1qOlhMl6ibZzzv3lL39x5eXlzu/3u7KyMtfa2mo9kploNOpqa2tdcXGxy8zMdLNnz3YbN250sVjMerSk++c//znma8SqVaucc19fpv3KK6+4/Px85/f73YMPPuiOHz/u+Rx8lAMAwMyEek8IAJBeiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAz/wfrCqDqe+eOoQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the Environment class\n",
    "env = Environment(environment_rows, environment_columns, ORDER_REWARD, TERMINAL_PENALTY, STEP_PENALTY, START, reward_locs)\n",
    "\n",
    "# You can access the rewards array using env.rewards\n",
    "plt.imshow(env.active_rewrads, cmap='hot', interpolation='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.is_terminal_state(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Type\n",
    "from tqdm import tqdm\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, NUM_ORDERS, environment: Type[Environment],environment_rows, environment_columns, actions=['up', 'right', 'down', 'left']):\n",
    "        self.env = environment\n",
    "        self.q_values = np.zeros((NUM_ORDERS, environment_rows, environment_columns, 4))\n",
    "        self.NUM_ORDERS = NUM_ORDERS\n",
    "        self.environment_rows = environment_rows\n",
    "        self.environment_columns = environment_columns\n",
    "        self.actions = actions\n",
    "\n",
    "\n",
    "    def get_next_action(self, current_row_index, current_column_index, epsilon, order_number):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.argmax(self.q_values[order_number, current_row_index, current_column_index])\n",
    "        else:\n",
    "            return np.random.randint(4)\n",
    "\n",
    "    def get_next_location(self, current_row_index, current_column_index, action_index):\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.environment_columns - 1:\n",
    "            new_column_index += 1\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.environment_rows - 1:\n",
    "            new_row_index += 1\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "        else:\n",
    "            raise Exception(f\"Invalid action {action_index}\")\n",
    "        return new_row_index, new_column_index\n",
    "    \n",
    "    def get_shortest_path(self):\n",
    "        self.env.reset_active_rewards()\n",
    "        shortest_path = []\n",
    "        if self.env.is_terminal_state(self.env.START[0], self.env.START[1]) or self.env.is_order_state(self.env.START[0], self.env.START[1]):\n",
    "            raise Exception(\"Cannot find shortest path from terminal state or order state, start from a different location\")\n",
    "        current_row_index, current_column_index = self.env.get_starting_location()\n",
    "        shortest_path.append([current_row_index, current_column_index])\n",
    "        while not self.env.is_terminal_state(current_row_index, current_column_index) and self.env.orders_left > 0:\n",
    "            order_number = self.env.NUM_ORDERS - self.env.orders_left # order number is the number of orders completed so far which is the index of the current order and defines the q_values layer to use\n",
    "            action_index = self.get_next_action(current_row_index, current_column_index,1.0, order_number)\n",
    "            current_row_index, current_column_index = self.get_next_location(current_row_index, current_column_index, action_index)\n",
    "            shortest_path.append([current_row_index, current_column_index])\n",
    "            self.env.is_order_state(current_row_index, current_column_index)\n",
    "            \n",
    "            if len(shortest_path) > 1000:\n",
    "                print(\"Infinite loop! Something is wrong\")\n",
    "                break\n",
    "        return shortest_path\n",
    "    \n",
    "    def train(self, num_episodes, epsilon, discount_factor, learning_rate):\n",
    "        gamma = 1-(1 / (num_episodes/2))\n",
    "        progress_bar = tqdm(range(num_episodes), desc=\"Training Progress\", unit=\"Episode\")\n",
    "        zero_order_hit_count = 0\n",
    "        one_order_hit_count = 0\n",
    "        episode_rewards_list = []\n",
    "        for episode in progress_bar:\n",
    "            self.env.reset_active_rewards()\n",
    "            episode_rewards_sum = 0\n",
    "            row_index, column_index = self.env.get_starting_location()\n",
    "            \n",
    "            while not self.env.is_terminal_state(row_index, column_index) and self.env.orders_left > 0:\n",
    "                order_number = self.env.NUM_ORDERS - self.env.orders_left\n",
    "                action_index = self.get_next_action(row_index, column_index,epsilon, order_number=order_number)\n",
    "                \n",
    "                old_row_index, old_column_index = row_index, column_index # store the old row and column indexes\n",
    "                row_index, column_index = self.get_next_location(row_index, column_index, action_index)\n",
    "                \n",
    "                reward = self.env.active_rewrads[row_index, column_index]\n",
    "                episode_rewards_sum += reward\n",
    "                \n",
    "                old_q_value = self.q_values[order_number, old_row_index, old_column_index, action_index]\n",
    "                temporal_difference = reward + (discount_factor * np.max(self.q_values[order_number, row_index, column_index])) - old_q_value\n",
    "                new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "                self.q_values[order_number, old_row_index, old_column_index, action_index] = new_q_value\n",
    "                \n",
    "                if self.env.is_order_state(row_index, column_index):\n",
    "                    if self.env.orders_left == 0:\n",
    "                        zero_order_hit_count += 1\n",
    "                    elif self.env.orders_left == 1:\n",
    "                        one_order_hit_count += 1\n",
    "                \n",
    "            episode_rewards_list.append(episode_rewards_sum)\n",
    "            \n",
    "            learning_rate = max(learning_rate * gamma, 0.01)\n",
    "            epsilon = min(epsilon * (2-gamma), 0.9)\n",
    "\n",
    "            progress_bar.set_postfix({\"Ord left\": self.env.orders_left, \"lr\": learning_rate, \"Eps\": epsilon,\n",
    "                                      \"0 Order\":zero_order_hit_count, \"1 Order\":one_order_hit_count})\n",
    "        \n",
    "        progress_bar.close()\n",
    "        return episode_rewards_list\n",
    "            \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 11, 11, 4)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = QLearningAgent(env.NUM_ORDERS, env,env.environment_rows, env.environment_columns)\n",
    "agent.q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 10000/10000 [00:07<00:00, 1303.27Episode/s, Ord left=2, lr=0.122, Eps=0.9, 0 Order=2004, 1 Order=3292]\n"
     ]
    }
   ],
   "source": [
    "reward_per_episode_list = agent.train(num_episodes=10000, epsilon=0.9, discount_factor=0.9, learning_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_path = agent.get_shortest_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 6],\n",
       " [1, 7],\n",
       " [2, 7],\n",
       " [1, 7],\n",
       " [1, 6],\n",
       " [1, 5],\n",
       " [2, 5],\n",
       " [3, 5],\n",
       " [4, 5],\n",
       " [5, 5],\n",
       " [5, 4],\n",
       " [6, 4],\n",
       " [6, 3],\n",
       " [6, 2],\n",
       " [6, 1],\n",
       " [7, 1],\n",
       " [8, 1],\n",
       " [8, 2],\n",
       " [9, 2],\n",
       " [9, 3],\n",
       " [9, 4],\n",
       " [9, 5],\n",
       " [9, 6],\n",
       " [9, 7],\n",
       " [8, 7],\n",
       " [9, 7],\n",
       " [9, 6],\n",
       " [9, 5],\n",
       " [8, 5],\n",
       " [7, 5],\n",
       " [6, 5],\n",
       " [5, 5],\n",
       " [4, 5],\n",
       " [3, 5],\n",
       " [2, 5],\n",
       " [1, 5]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
